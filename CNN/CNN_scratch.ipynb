{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN From Scratch Using NumPy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "np.seterr(all='ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def d_relu(X):\n",
    "    return 1. * (X > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(X, K, stride):\n",
    "    N, C, H, W = X.shape\n",
    "    C, kernel_height, kernel_width = K.shape\n",
    "    num_filters = 1\n",
    "    \n",
    "    if ((int) ((H - kernel_height) % stride) != 0) or ((int) ((W - kernel_width) % stride) != 0):\n",
    "        print(\"Invalid dimension for convolution\")\n",
    "        return\n",
    "\n",
    "    out_height = (int) ((H - kernel_height) / stride) + 1\n",
    "    out_width = (int) ((W - kernel_width) / stride) + 1\n",
    "    \n",
    "    def conv_f(N, num_filters, i, j):\n",
    "        i = int(i)\n",
    "        j = int(j)\n",
    "        N = int(N)\n",
    "        A = X[N, :, i*stride : i*stride + kernel_height, j*stride : j*stride + kernel_width]\n",
    "        t = np.multiply(A, K)\n",
    "        #Sum the result across all channels\n",
    "        t1 = np.sum(t, axis=0)\n",
    "        #Sum of all elements of the matrix\n",
    "        out = t1.sum()\n",
    "        return out\n",
    "\n",
    "    f = np.vectorize(conv_f)\n",
    "    Z = np.fromfunction(f, (N, num_filters, out_height, out_width) )\n",
    "    \n",
    "    #Save information in cache for the backprop\n",
    "    cache = (X, K, stride)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def max_pool_forward(X, size, stride):\n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    if ((int) ((H - size) % stride) != 0) or ((int) ((W - size) % stride) != 0):\n",
    "        print(\"Invalid dimension for convolution\")\n",
    "        return\n",
    "    \n",
    "    out_height = (int) ((H - size) / stride) + 1\n",
    "    out_width = (int) ((W - size) / stride) + 1\n",
    "\n",
    "    def pool_f(N, C, i, j):\n",
    "        i = int(i)\n",
    "        j = int(j)\n",
    "        N = int(N)\n",
    "        A = X[N, :, i*stride : i*stride + size, j*stride : j*stride + size]\n",
    "        out = np.amax(A)\n",
    "        return out\n",
    "\n",
    "    f = np.vectorize(pool_f)\n",
    "    output_pool = np.fromfunction(f, (N, C, out_height, out_width) )\n",
    "    \n",
    "    #Save information in cache for the backprop\n",
    "    cache = (X, size, stride) \n",
    "    \n",
    "    return output_pool, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    mask = (x == np.max(x))\n",
    "    return mask\n",
    "\n",
    "def pool_backward(dA, cache):\n",
    "    A_prev, size, stride = cache\n",
    "    m, n_C_prev , n_H_prev, n_W_prev = A_prev.shape\n",
    "    m, n_C, n_H, n_W = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    def pool_b(N,C,i,j):\n",
    "        i = int(i)\n",
    "        j = int(j)\n",
    "        C = int(C)\n",
    "        N = int(N)\n",
    "        a_prev_slice = A_prev[N, :, i * stride : i * stride + size, j * stride : j * stride + size]\n",
    "        mask = create_mask_from_window(a_prev_slice)\n",
    "        dA_prev[N, :, i * stride : i * stride + size, j * stride : j * stride + size] += np.multiply(mask, dA[N, C, i, j])\n",
    "        \n",
    "    b = np.vectorize(pool_b)\n",
    "    np.fromfunction(b, shape=(dA.shape))\n",
    "   \n",
    "    return dA_prev\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    A_prev, W, stride = cache\n",
    "    C, f, f = W.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)                           \n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    def conv_b(N, C, i, j):\n",
    "        i = int(i)\n",
    "        j = int(j)\n",
    "        N = int(N)\n",
    "        C = int(C)\n",
    "        a_slice = A_prev[N, :, i * stride : i * stride + f, j * stride : j * stride + f]\n",
    "        dA_prev[N, :, i * stride : i * stride + f, j * stride : j * stride + f] += W[C, :, :] * dZ[N, C, i, j]\n",
    "        dW[:,:,:] += a_slice * dZ[N, C, i, j]\n",
    "    \n",
    "    b = np.vectorize(conv_b)\n",
    "    np.fromfunction(b, shape=(dZ.shape))\n",
    "\n",
    "    return dA_prev, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 3\n",
    "# Weight of convolution layer 1\n",
    "k1 = [[1, 4, 6, 4, 1],\n",
    "      [4, 16, 24, 16, 4],\n",
    "      [6, 24, 36, 24, 6],\n",
    "      [4, 16, 24, 16, 4],\n",
    "      [1, 4, 6, 4, 1]]\n",
    "np.divide(k1,256)\n",
    "K1 = np.tile(k1, (C, 1))\n",
    "K1 = K1.reshape(C, len(k1), len(k1[0]))\n",
    "# Stride of convolution layer 1\n",
    "stride_c1 = 1\n",
    "# Size of Pool layer 1\n",
    "size1 = 2\n",
    "# Stride of Pooling layer 1\n",
    "stride_p1 = 2\n",
    "\n",
    "C = 1\n",
    "# Weight of convolution layer 2\n",
    "k2 = [[0, -1, 0],\n",
    "      [-1, 5, -1],\n",
    "      [0, -1, 0]]\n",
    "K2 = np.tile(k2, (C, 1))\n",
    "K2 = K2.reshape(C, len(k2), len(k2[0]))\n",
    "# Stride of convolution layer 2\n",
    "stride_c2 = 1\n",
    "# Size of Pool layer 1\n",
    "size2 = 2\n",
    "# Stride of Pooling layer 1\n",
    "stride_p2 = 1\n",
    "\n",
    "# Fully Connected Layer length\n",
    "num_fc = 64\n",
    "out_height = 11\n",
    "out_width = 11\n",
    "# Weights of fully connected layer\n",
    "W3 = np.random.standard_normal(size=(out_height*out_width, num_fc))\n",
    "# Bias of fully connected layer\n",
    "b3 = np.random.standard_normal(size=num_fc)\n",
    "\n",
    "# Output Layer length\n",
    "num_outputs = 10\n",
    "# Weights of Softmax layer \n",
    "W4 = np.random.standard_normal(size=(num_fc, num_outputs))\n",
    "# Bias of softmax layer\n",
    "b4 = np.random.standard_normal(size=num_outputs)\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "# CONV -> RELU -> MAXPOOL -> CONV -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED -> SOFTMAX\n",
    "\n",
    "def model(X, Y):\n",
    "    ###### LAYER 1 ######\n",
    "    N, C, H, W = X.shape\n",
    "\n",
    "    X_conv1, cache_conv1 = conv_forward(X, K1, stride_c1)\n",
    "    print(\"Convolved shape 1: \", X_conv1.shape)\n",
    "\n",
    "    X_activation1 = relu(X_conv1)\n",
    "    print(\"Activated shape 1: \", X_activation1.shape)\n",
    "\n",
    "    X_pool1, cache_pool1 = max_pool_forward(X_activation1, size1, stride_p1)\n",
    "    print(\"Pooled shape 1: \", X_pool1.shape)\n",
    "\n",
    "\n",
    "    ###### LAYER 2 ######\n",
    "    N, C, H, W = X_pool1.shape\n",
    "\n",
    "    X_conv2, cache_conv2 = conv_forward(X_pool1, K2, stride_c2)\n",
    "    print(\"Convolved shape 2: \", X_conv2.shape)\n",
    "\n",
    "    X_activation2 = relu(X_conv2)\n",
    "    print(\"Activated shape 2: \", X_activation2.shape)\n",
    "\n",
    "    X_pool2, cache_pool2 = max_pool_forward(X_activation2, size2, stride_p2)\n",
    "    print(\"Pooled shape 2: \", X_pool2.shape)\n",
    "\n",
    "\n",
    "    ###### Fully Connected + Softmax Layer ######\n",
    "    N, C, out_height, out_width = X_pool2.shape\n",
    "\n",
    "    X_flat = X_pool2.reshape(N, out_height*out_width)\n",
    "    print(\"Flattened shape: \" , np.array(X_flat.shape))\n",
    "\n",
    "    X_fc = np.dot(X_flat, W3) + b3\n",
    "    print(\"FC shape: \", X_fc.shape)\n",
    "\n",
    "    X_fc_act = relu(X_fc)\n",
    "    print(\"Activated shape: \", X_fc_act.shape)\n",
    "\n",
    "    y_pred = np.dot(X_fc_act, W4) + b4\n",
    "    print(\"y_pred shape: \", y_pred.shape)\n",
    "    \n",
    "    cache = (cache_conv1, cache_pool1, cache_conv2, cache_pool2, X_flat, X_fc_act)\n",
    "        \n",
    "    return y_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Calculate Loss ######\n",
    "def calculate_loss(N, y_pred, Y):\n",
    "    exp_scores = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True), casting=\"unsafe\")\n",
    "\n",
    "    # Softmax activation\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Log loss of the correct class of each of samples\n",
    "    epsilon = 1e-4\n",
    "    correct_logprobs = -np.log(probs[np.arange(N), Y] + epsilon)\n",
    "\n",
    "    # Compute the average loss\n",
    "    loss = np.sum(correct_logprobs) / N\n",
    "\n",
    "    return loss, probs\n",
    "\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### BackProp ######\n",
    "def backprop(N, probs, cache, Y):\n",
    "    cache_conv1, cache_pool1, cache_conv2, cache_pool2, X_flat, X_fc_act = cache\n",
    "    delta4 = probs\n",
    "    delta4[range(N), Y] -= 1\n",
    "\n",
    "    dW4 = (d_relu(X_fc_act).T).dot(delta4)\n",
    "    db4 = np.sum(delta4, axis=0, keepdims=True).reshape(-1)\n",
    "    print(\"Weights shape softmax layer: \", dW4.shape, db4.shape)\n",
    "\n",
    "    global W4\n",
    "    global W3\n",
    "    global b4\n",
    "    global b3\n",
    "    global K2\n",
    "    global K1\n",
    "    delta3 = delta4.dot(W4.T) * (1 - np.power(X_fc_act, 2))\n",
    "\n",
    "    dW3 = np.dot(X_flat.T, delta3)\n",
    "    db3 = np.sum(delta3, axis=0, keepdims=True).reshape(-1)\n",
    "    print(\"Weights shape FC layer: \", dW3.shape, db3.shape)\n",
    "\n",
    "    delta2 = delta3.dot(W3.T) * (1 - np.power(X_flat, 2))\n",
    "    delta2 = delta2.reshape(N, 1, out_height, out_width)\n",
    "\n",
    "    da_pool2_prev = pool_backward(delta2, cache_pool2)\n",
    "    print(\"Pooled layer 2 shape: \", da_pool2_prev.shape)\n",
    "\n",
    "    da_conv2_prev, dK2 = conv_backward(da_pool2_prev, cache_conv2)\n",
    "    print(\"Convolved layer 2 shape: \", da_conv2_prev.shape)\n",
    "    print(\"Convolved layer 2 weight shape: \", dK2.shape)\n",
    "\n",
    "    da_pool1_prev = pool_backward(da_conv2_prev, cache_pool1)\n",
    "    print(\"Pooled layer 1 shape: \", da_pool1_prev.shape)\n",
    "\n",
    "    da_conv1_prev, dK1 = conv_backward(da_pool1_prev, cache_conv1)\n",
    "    print(\"Convolved layer 1 shape: \", da_conv1_prev.shape)\n",
    "    print(\"Convolved layer 1 weight shape: \", dK1.shape)\n",
    "\n",
    "    # Gradient descent parameter update\n",
    "    W4 -= eta * dW4\n",
    "    b4 -= eta * db4\n",
    "    W3 -= eta * dW3\n",
    "    b3 -= eta * db3\n",
    "    \n",
    "    np.subtract(K2, np.multiply(dK2, eta, casting='unsafe'), out = K2, casting = 'unsafe')\n",
    "    np.subtract(K1, np.multiply(dK1, eta, casting='unsafe'), out = K1, casting = 'unsafe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "Feed Forward:\n",
      "Convolved shape 1:  (10000, 1, 28, 28)\n",
      "Activated shape 1:  (10000, 1, 28, 28)\n",
      "Pooled shape 1:  (10000, 1, 14, 14)\n",
      "Convolved shape 2:  (10000, 1, 12, 12)\n",
      "Activated shape 2:  (10000, 1, 12, 12)\n",
      "Pooled shape 2:  (10000, 1, 11, 11)\n",
      "Flattened shape:  [10000   121]\n",
      "FC shape:  (10000, 64)\n",
      "Activated shape:  (10000, 64)\n",
      "y_pred shape:  (10000, 10)\n",
      "Batch 1 Loss  8.283770071058346\n",
      "Back Propogation:\n",
      "Weights shape softmax layer:  (64, 10) (10,)\n",
      "Weights shape FC layer:  (121, 64) (64,)\n",
      "Pooled layer 2 shape:  (10000, 1, 12, 12)\n",
      "Convolved layer 2 shape:  (10000, 1, 14, 14)\n",
      "Convolved layer 2 weight shape:  (1, 3, 3)\n",
      "Pooled layer 1 shape:  (10000, 1, 28, 28)\n",
      "Convolved layer 1 shape:  (10000, 3, 32, 32)\n",
      "Convolved layer 1 weight shape:  (3, 5, 5)\n",
      "Epoch  2\n",
      "Feed Forward:\n",
      "Convolved shape 1:  (10000, 1, 28, 28)\n",
      "Activated shape 1:  (10000, 1, 28, 28)\n",
      "Pooled shape 1:  (10000, 1, 14, 14)\n",
      "Convolved shape 2:  (10000, 1, 12, 12)\n",
      "Activated shape 2:  (10000, 1, 12, 12)\n",
      "Pooled shape 2:  (10000, 1, 11, 11)\n",
      "Flattened shape:  [10000   121]\n",
      "FC shape:  (10000, 64)\n",
      "Activated shape:  (10000, 64)\n",
      "y_pred shape:  (10000, 10)\n",
      "Batch 2 Loss  8.265349190324391\n",
      "Back Propogation:\n",
      "Weights shape softmax layer:  (64, 10) (10,)\n",
      "Weights shape FC layer:  (121, 64) (64,)\n",
      "Pooled layer 2 shape:  (10000, 1, 12, 12)\n",
      "Convolved layer 2 shape:  (10000, 1, 14, 14)\n",
      "Convolved layer 2 weight shape:  (1, 3, 3)\n",
      "Pooled layer 1 shape:  (10000, 1, 28, 28)\n",
      "Convolved layer 1 shape:  (10000, 3, 32, 32)\n",
      "Convolved layer 1 weight shape:  (3, 5, 5)\n",
      "Epoch  3\n",
      "Feed Forward:\n",
      "Convolved shape 1:  (10000, 1, 28, 28)\n",
      "Activated shape 1:  (10000, 1, 28, 28)\n",
      "Pooled shape 1:  (10000, 1, 14, 14)\n",
      "Convolved shape 2:  (10000, 1, 12, 12)\n",
      "Activated shape 2:  (10000, 1, 12, 12)\n",
      "Pooled shape 2:  (10000, 1, 11, 11)\n",
      "Flattened shape:  [10000   121]\n",
      "FC shape:  (10000, 64)\n",
      "Activated shape:  (10000, 64)\n",
      "y_pred shape:  (10000, 10)\n",
      "Batch 3 Loss  8.262586058214302\n",
      "Back Propogation:\n",
      "Weights shape softmax layer:  (64, 10) (10,)\n",
      "Weights shape FC layer:  (121, 64) (64,)\n",
      "Pooled layer 2 shape:  (10000, 1, 12, 12)\n",
      "Convolved layer 2 shape:  (10000, 1, 14, 14)\n",
      "Convolved layer 2 weight shape:  (1, 3, 3)\n",
      "Pooled layer 1 shape:  (10000, 1, 28, 28)\n",
      "Convolved layer 1 shape:  (10000, 3, 32, 32)\n",
      "Convolved layer 1 weight shape:  (3, 5, 5)\n",
      "Epoch  4\n",
      "Feed Forward:\n",
      "Convolved shape 1:  (10000, 1, 28, 28)\n",
      "Activated shape 1:  (10000, 1, 28, 28)\n",
      "Pooled shape 1:  (10000, 1, 14, 14)\n",
      "Convolved shape 2:  (10000, 1, 12, 12)\n",
      "Activated shape 2:  (10000, 1, 12, 12)\n",
      "Pooled shape 2:  (10000, 1, 11, 11)\n",
      "Flattened shape:  [10000   121]\n",
      "FC shape:  (10000, 64)\n",
      "Activated shape:  (10000, 64)\n",
      "y_pred shape:  (10000, 10)\n",
      "Batch 4 Loss  8.285612159131741\n",
      "Back Propogation:\n",
      "Weights shape softmax layer:  (64, 10) (10,)\n",
      "Weights shape FC layer:  (121, 64) (64,)\n",
      "Pooled layer 2 shape:  (10000, 1, 12, 12)\n",
      "Convolved layer 2 shape:  (10000, 1, 14, 14)\n",
      "Convolved layer 2 weight shape:  (1, 3, 3)\n",
      "Pooled layer 1 shape:  (10000, 1, 28, 28)\n",
      "Convolved layer 1 shape:  (10000, 3, 32, 32)\n",
      "Convolved layer 1 weight shape:  (3, 5, 5)\n",
      "Epoch  5\n",
      "Feed Forward:\n",
      "Convolved shape 1:  (10000, 1, 28, 28)\n",
      "Activated shape 1:  (10000, 1, 28, 28)\n",
      "Pooled shape 1:  (10000, 1, 14, 14)\n",
      "Convolved shape 2:  (10000, 1, 12, 12)\n",
      "Activated shape 2:  (10000, 1, 12, 12)\n",
      "Pooled shape 2:  (10000, 1, 11, 11)\n",
      "Flattened shape:  [10000   121]\n",
      "FC shape:  (10000, 64)\n",
      "Activated shape:  (10000, 64)\n",
      "y_pred shape:  (10000, 10)\n",
      "Batch 5 Loss  8.274559630691371\n",
      "Back Propogation:\n",
      "Weights shape softmax layer:  (64, 10) (10,)\n",
      "Weights shape FC layer:  (121, 64) (64,)\n",
      "Pooled layer 2 shape:  (10000, 1, 12, 12)\n",
      "Convolved layer 2 shape:  (10000, 1, 14, 14)\n",
      "Convolved layer 2 weight shape:  (1, 3, 3)\n",
      "Pooled layer 1 shape:  (10000, 1, 28, 28)\n",
      "Convolved layer 1 shape:  (10000, 3, 32, 32)\n",
      "Convolved layer 1 weight shape:  (3, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch \", i+1)\n",
    "    # Load dataset batch i\n",
    "    f = open(\"./data/cifar-10-batches-py/data_batch_{}\".format(i+1), 'rb')\n",
    "    datadict = cPickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    X_raw = datadict[\"data\"]\n",
    "    Y = datadict['labels']\n",
    "    X = X_raw.reshape(10000, 3, 32, 32)\n",
    "    batch_size = 10000\n",
    "    print(\"Feed Forward:\")\n",
    "    y_pred, cache = model(X, Y)\n",
    "    loss, probs = calculate_loss(batch_size, y_pred, Y)\n",
    "    print(\"Batch {} Loss \".format(i+1), loss)\n",
    "    print(\"Back Propogation:\")\n",
    "    backprop(batch_size, probs, cache, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolved shape 1:  (10000, 1, 28, 28)\n",
      "Activated shape 1:  (10000, 1, 28, 28)\n",
      "Pooled shape 1:  (10000, 1, 14, 14)\n",
      "Convolved shape 2:  (10000, 1, 12, 12)\n",
      "Activated shape 2:  (10000, 1, 12, 12)\n",
      "Pooled shape 2:  (10000, 1, 11, 11)\n",
      "Flattened shape:  [10000   121]\n",
      "FC shape:  (10000, 64)\n",
      "Activated shape:  (10000, 64)\n",
      "y_pred shape:  (10000, 10)\n",
      "Test Data Loss:  8.289296335278532\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy with test dataset\n",
    "f = open(\"./data/cifar-10-batches-py/test_batch\", 'rb')\n",
    "datadict = cPickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "X_raw = datadict[\"data\"]\n",
    "Y_test = datadict['labels']\n",
    "X_test = X_raw.reshape(10000, 3, 32, 32)\n",
    "batch_size = 10000\n",
    "y_pred, cache = model(X_test, Y_test)\n",
    "loss, probs = calculate_loss(batch_size, y_pred, Y_test)\n",
    "print(\"Test Data Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  10.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy : \",accuracy_score(Y_test, np.argmax(y_pred, axis=1))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
